{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.- ***Instalación y configuración.***"
      ],
      "metadata": {
        "id": "yuHhwygL5QwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TTS es compatible con Python 3.7 a <3.11.0 y ha sido probado en Ubuntu 18.10, 19.10 y 20.10."
      ],
      "metadata": {
        "id": "NQimfufp5lTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Instalación desde PyPI*"
      ],
      "metadata": {
        "id": "MfVRflOJ5x3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar TTS desde PyPI\n",
        "!pip install TTS\n"
      ],
      "metadata": {
        "id": "7CajQeUQ5ydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Instalación desde Github*"
      ],
      "metadata": {
        "id": "Gozhrdi35qUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar TTS desde GitHub\n",
        "!pip install git+https://github.com/coqui-ai/TTS\n"
      ],
      "metadata": {
        "id": "EV-2mrTS5WOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Instalación desde sistemas Linux. *"
      ],
      "metadata": {
        "id": "W1flAvzE55PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar el repositorio desde GitHub\n",
        "!git clone https://github.com/coqui-ai/TTS/\n",
        "\n",
        "# Navegar al directorio del repositorio\n",
        "%cd TTS\n",
        "\n",
        "# Instalar las dependencias del sistema (solo en Linux)\n",
        "!make system-deps\n",
        "\n",
        "# Instalar TTS\n",
        "!make install\n"
      ],
      "metadata": {
        "id": "SFSR-l6L56xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PE1sKgrm5-9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2.-Configuración y Ejecución del Entrenamiento del Modelo***"
      ],
      "metadata": {
        "id": "eSbeevT66BBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Importaciones y Configuración de Rutas:\n",
        "\n",
        "2. Definir Configuración del Dataset:\n",
        "3. Inicializar el Procesador de Audio:\n",
        "4. Inicializar el Tokenizador:\n",
        "5. Cargar Muestras de Datos:\n",
        "6. Inicializar el Modelo:\n",
        "7. Inicializar el Entrenador:\n",
        "8. Entrenamiento\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "whgd7bUN8L-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Importar módulos necesarios\n",
        "from trainer import Trainer, TrainerArgs\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "# Definir la ruta de salida como la misma que el script\n",
        "output_path = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "# Definir la configuración del dataset\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n",
        ")\n",
        "\n",
        "# Inicializar el procesador de audio\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "\n",
        "# Inicializar el tokenizador\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# Cargar muestras de datos para entrenamiento y evaluación\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "# Inicializar la configuración del modelo\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=1000,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        ")\n",
        "\n",
        "# Inicializar el modelo\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n",
        "\n",
        "# Inicializar el entrenador\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")\n",
        "\n",
        "# Iniciar el entrenamiento\n",
        "trainer.fit()\n"
      ],
      "metadata": {
        "id": "1HZjrL5e8BeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3.Parámetros de Preprocesamiento y Ejemplo de Audio***"
      ],
      "metadata": {
        "id": "41x1LMJp-Agn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aspectos a considerar.\n",
        "\n",
        "  **Tiempo:** ¿Cuál es la duración del audio que se considera adecuada? ¿Hay alguna restricción en la duración?\n",
        "\n",
        "  **Frecuencia:** ¿Cuál es la frecuencia de muestreo necesaria para el modelo? ¿Cómo asegurarse de que todos los audios tengan esta frecuencia?\n",
        "    \n",
        "  **Transformaciones:** ¿Qué transformaciones son necesarias para normalizar el audio? ¿Se necesita una transformación lineal o alguna otra técnica de ajuste?\n",
        "  \n",
        "  **Longitud de los Segmentos de Audio:**\n",
        "  Algunos modelos requieren que los audios se dividan en segmentos de longitud fija. Especifica si se debe cortar o rellenar el audio para que se ajuste a esta longitud.\n",
        "\n",
        "\n",
        "**Ventanas y Superposición:**\n",
        "  Si se utilizan técnicas como la ventana de Fourier para la transformación del audio, menciona el tamaño de la ventana y el grado de superposición.\n",
        "\n",
        "**Especificaciones de Formato:**\n",
        "\n",
        "  Algunas veces el modelo puede tener requisitos específicos sobre el formato del archivo (por ejemplo, WAV, FLAC) o sobre la profundidad de bits.\n",
        "\n",
        "**Normalización del Volumen:**\n",
        "\n",
        "  Describe cómo ajustar el volumen del audio para asegurar que todos los archivos tengan un nivel de audio consistente.\n",
        "\n",
        "**Ruido de Fondo y Limpieza:**\n",
        "\n",
        "  Si el modelo requiere que el audio esté libre de ruido de fondo, menciona cualquier proceso de limpieza que deba aplicarse.\n",
        "\n",
        "**Padding y Trimming:**\n",
        "\n",
        "  Explica cómo se deben manejar los audios que son demasiado cortos (relleno) o demasiado largos (recorte).\n",
        "\n",
        "**Especificaciones del Texto:**\n",
        "\n",
        "  Si el modelo también necesita que el texto esté en un formato específico (por ejemplo, sin puntuación, en minúsculas), incluye detalles sobre cómo preparar y formatear el texto."
      ],
      "metadata": {
        "id": "l0jR_Zou-GuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Preprocesamiento de Audio en Python"
      ],
      "metadata": {
        "id": "f6ALUCbA-XDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.models.vits import Vits\n",
        "from TTS.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "# Configuración del modelo VITS\n",
        "config = VitsConfig(\n",
        "    output_path='output',  # Ruta donde se guardarán los resultados del entrenamiento\n",
        "    run_name='run',  # Nombre del experimento\n",
        "    model='vits',  # Tipo de modelo\n",
        "    epochs=1000,  # Número total de épocas para el entrenamiento\n",
        "    batch_size=32,  # Tamaño del lote de datos para el entrenamiento\n",
        "    lr=0.001,  # Tasa de aprendizaje para el optimizador\n",
        "    optimizer='AdamW',  # Tipo de optimizador a utilizar\n",
        "    save_step=10000,  # Número de pasos después del cual guardar el modelo\n",
        "    save_checkpoints=True,  # Si se deben guardar puntos de control durante el entrenamiento\n",
        "    audio={\n",
        "        'sample_rate': 22050,  # Frecuencia de muestreo del audio (ejemplo: 22050 Hz)\n",
        "        'n_fft': 2048,  # Tamaño de la ventana para la transformada de Fourier\n",
        "        'hop_size': 512,  # Tamaño del salto entre ventanas\n",
        "        'win_size': 2048,  # Tamaño de la ventana de análisis\n",
        "        'preemphasis': 0.97,  # Coeficiente de preénfasis para mejorar la relación señal-ruido\n",
        "        'min_db': -100,  # Umbral mínimo en decibelios para normalización\n",
        "        'min_level_db': -100  # Umbral mínimo en decibelios para el audio\n",
        "    }\n",
        ")\n",
        "\n",
        "# Inicialización del Procesador de Audio\n",
        "ap = AudioProcessor(**config.audio.to_dict())\n",
        "\n",
        "# Inicialización del Tokenizador\n",
        "tokenizer, _ = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# Función para preprocesar audio\n",
        "def preprocess_audio(file_path, config):\n",
        "    # Cargar audio\n",
        "    y, sr = librosa.load(file_path, sr=config.audio['sample_rate'])\n",
        "\n",
        "    # Normalización del volumen\n",
        "    y = y / np.max(np.abs(y))\n",
        "\n",
        "    # Transformaciones: Preénfasis\n",
        "    y = np.append(y[0], y[1:] - config.audio['preemphasis'] * y[:-1])\n",
        "\n",
        "    # Padding y Trimming\n",
        "    target_length = config.audio['n_fft']\n",
        "    if len(y) < target_length:\n",
        "        y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
        "    else:\n",
        "        y = y[:target_length]\n",
        "\n",
        "    return y\n",
        "\n",
        "# Cargar y preprocesar datos\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\",\n",
        "    meta_file_train=\"metadata.csv\",\n",
        "    path=os.path.join(config.output_path, \"../LJSpeech-1.1/\")\n",
        ")\n",
        "\n",
        "# Preprocesar un archivo de ejemplo\n",
        "example_audio_path = os.path.join(config.output_path, \"../LJSpeech-1.1/wavs/example.wav\")\n",
        "preprocessed_audio = preprocess_audio(example_audio_path, config)\n",
        "\n",
        "# Cargar muestras de entrenamiento\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "# Inicialización del Modelo VITS\n",
        "model = Vits(config, ap, tokenizer)\n",
        "\n",
        "# Inicialización del Entrenador\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, config.output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")\n",
        "\n",
        "# Ejecución del Entrenamiento\n",
        "trainer.fit()\n",
        "\n",
        "# Evaluación del Modelo\n",
        "trainer.evaluate()\n",
        "\n",
        "# Exportación del Modelo\n",
        "model.export_onnx(output_path='coqui_vits.onnx')\n"
      ],
      "metadata": {
        "id": "NQMHkeZe-85V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiempo:**\n",
        "\n",
        "  Duración del audio: Se maneja en la función preprocess_audio con el padding y trimming, asegurando que todos los audios sean de la longitud esperada.\n",
        "\n",
        "**Frecuencia:**\n",
        "\n",
        "  Frecuencia de muestreo: Especificada en config.audio['sample_rate'] para asegurar que todos los audios tengan esta frecuencia.\n",
        "\n",
        "**Transformaciones:**\n",
        "\n",
        "  Preénfasis: Se aplica en la función preprocess_audio para mejorar la relación señal-ruido.\n",
        "\n",
        "**Longitud de los Segmentos de Audio:**\n",
        "\n",
        "  Padding y Trimming: Ajusta la longitud del audio en la función preprocess_audio para que se ajuste a la longitud especificada.\n",
        "\n",
        "**Ventanas y Superposición:**\n",
        "\n",
        "  Tamaño de la ventana: Configurado con config.audio['n_fft'] y config.audio['hop_size'], afecta la transformada de Fourier.\n",
        "\n",
        "**Especificaciones de Formato:**\n",
        "\n",
        "  Formato del archivo: En este caso, se espera que el audio esté en formato WAV, pero puede ser ajustado según las necesidades del modelo.\n",
        "\n",
        "**Normalización del Volumen:**\n",
        "\n",
        "  Normalización: Se aplica en la función preprocess_audio para asegurar que el nivel de audio sea consistente.\n",
        "\n",
        "**Ruido de Fondo y Limpieza:**\n",
        "\n",
        "  Limpieza del audio: Aunque no se incluye un proceso de eliminación de ruido específico en este ejemplo, se puede agregar según los requisitos del modelo.\n",
        "\n",
        "**Padding y Trimming:**\n",
        "\n",
        "  Ajuste de longitud: Realizado en la función preprocess_audio para manejar audios demasiado cortos o largos.\n",
        "\n",
        "**Especificaciones del Texto:**\n",
        "\n",
        "  Preparación del texto: Asegúrate de que el texto esté en el formato necesario para el tokenizador, como se configura en TTSTokenizer."
      ],
      "metadata": {
        "id": "DCoA3JFRAve2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.- Ejemplo"
      ],
      "metadata": {
        "id": "Ift1jLz3CL9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Esta arquitectura ya la tenemos instalada\n",
        "from vits import VITSModel\n",
        "from utils import load_model, text_to_sequence\n",
        "\n",
        "# Paso 1: Cargar el modelo preentrenado\n",
        "model_path = \"vits_model.pth\"  # Ruta al modelo VITS preentrenado\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = load_model(model_path, device)\n",
        "\n",
        "# Paso 2: Convertir texto a secuencia de caracteres/fonemas\n",
        "text = \"Hola, ¿cómo estás?\" # Aqui podemos abrir los archivos de los textos de los audios\n",
        "sequence = text_to_sequence(text)\n",
        "\n",
        "# Paso 3: Convertir la secuencia en tensor\n",
        "text_tensor = torch.LongTensor(sequence).unsqueeze(0).to(device)\n",
        "\n",
        "# Paso 4: Generar la onda de audio usando el modelo VITS\n",
        "with torch.no_grad():\n",
        "    audio_output = model.inference(text_tensor)\n",
        "\n",
        "# Paso 5: Guardar la salida como un archivo de audio\n",
        "audio_output = audio_output.cpu().numpy()\n",
        "save_wav(audio_output, \"output.wav\")\n",
        "\n",
        "# Función para guardar el audio (por ejemplo en formato WAV)\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "def save_wav(audio, filename, sr=22050):\n",
        "    wav.write(filename, sr, (audio * 32767).astype('int16'))\n"
      ],
      "metadata": {
        "id": "BgVAXtvMCOBE",
        "outputId": "00621b0f-f708-4385-8974-5ef8a08ff05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-32267c9b6d0a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;31m# Esta arquitectura ya la tenemos instalada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvits\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVITSModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Paso 1: Cargar el modelo preentrenado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vits'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}