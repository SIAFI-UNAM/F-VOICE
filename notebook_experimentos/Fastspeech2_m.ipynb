{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar la ruta del directorio donde están los módulos\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hifigan\n",
    "from model import FastSpeech2, ScheduledOptim\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataset import Dataset\n",
    "\n",
    "model_yml =\"C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\config\\\\F-VOICE\\\\model.yaml\"\n",
    "preprocess_yml = \"C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\config\\\\F-VOICE\\\\preprocess.yaml\"\n",
    "train_yml = \"C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\config\\\\F-VOICE\\\\train.yaml\"\n",
    "model_config = yaml.load(open(model_yml, \"r\"), Loader=yaml.FullLoader)\n",
    "preprocess_config = yaml.load(open(preprocess_yml, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(train_yml, \"r\"), Loader=yaml.FullLoader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from text import text_to_sequence\n",
    "import numpy as np\n",
    "from utils.tools import pad_1D, pad_2D\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.tools import to_device,log,synth_one_sample\n",
    "\n",
    "#Modificar cmudict de acuerdo al diccionario generado de MFA\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, filename,dataset_name,preprocessed_path,cleaners,batch_size,sort=False, drop_last=False):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.preprocessed_path = preprocessed_path\n",
    "        self.cleaners = cleaners\n",
    "        self.batch_size = batch_size\n",
    "        self.basename, self.text, self.raw_text = self.process_meta(filename)\n",
    "        self.sort = sort\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.basename[idx]\n",
    "        raw_text = self.raw_text[idx]\n",
    "        phone = np.array(text_to_sequence(self.text[idx], self.cleaners))\n",
    "        mel_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"mel\",\n",
    "            \"{}-mel.npy\".format(basename),\n",
    "        )\n",
    "        mel = np.load(mel_path)\n",
    "        pitch_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"pitch\",\n",
    "            \"{}-pitch.npy\".format(basename),\n",
    "        )\n",
    "        pitch = np.load(pitch_path)\n",
    "        energy_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"energy\",\n",
    "            \"{}-energy.npy\".format(basename),\n",
    "        )\n",
    "        energy = np.load(energy_path)\n",
    "        duration_path = os.path.join(\n",
    "            self.preprocessed_path,\n",
    "            \"duration\",\n",
    "            \"{}-duration.npy\".format(basename),\n",
    "        )\n",
    "        duration = np.load(duration_path)\n",
    "\n",
    "        sample = {\n",
    "            \"id\": basename,\n",
    "            \"text\": phone,\n",
    "            \"raw_text\": raw_text,\n",
    "            \"mel\": mel,\n",
    "            \"pitch\": pitch,\n",
    "            \"energy\": energy,\n",
    "            \"duration\": duration,\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def process_meta(self, filename):\n",
    "        # Procesa el archivo de metadatos y devuelve las listas correspondientes\n",
    "        with open(\n",
    "            os.path.join(filename), \"r\", encoding=\"utf-8\"\n",
    "        ) as f:\n",
    "            name = []\n",
    "            text = []\n",
    "            raw_text = []\n",
    "            for line in f.readlines():\n",
    "                n, t, r = line.strip(\"\\n\").split(\"|\")\n",
    "                name.append(n)\n",
    "                text.append(t)\n",
    "                raw_text.append(r)\n",
    "            return name, text, raw_text\n",
    "    \n",
    "    def reprocess(self, data, idxs):\n",
    "        ids = [data[idx][\"id\"] for idx in idxs]\n",
    "        texts = [data[idx][\"text\"] for idx in idxs]\n",
    "        raw_texts = [data[idx][\"raw_text\"] for idx in idxs]\n",
    "        mels = [data[idx][\"mel\"] for idx in idxs]\n",
    "        pitches = [data[idx][\"pitch\"] for idx in idxs]\n",
    "        energies = [data[idx][\"energy\"] for idx in idxs]\n",
    "        durations = [data[idx][\"duration\"] for idx in idxs]\n",
    "\n",
    "        text_lens = np.array([text.shape[0] for text in texts])\n",
    "        mel_lens = np.array([mel.shape[0] for mel in mels])\n",
    "\n",
    "        texts = pad_1D(texts)\n",
    "        mels = pad_2D(mels)\n",
    "        pitches = pad_1D(pitches)\n",
    "        energies = pad_1D(energies)\n",
    "        durations = pad_1D(durations)\n",
    "        return (\n",
    "            ids,\n",
    "            raw_texts,\n",
    "            texts,\n",
    "            text_lens,\n",
    "            max(text_lens),\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            max(mel_lens),\n",
    "            pitches,\n",
    "            energies,\n",
    "            durations,\n",
    "        )\n",
    "    def collate_fn(self, data):\n",
    "        # Función para agrupar los datos en lotes\n",
    "        data_size = len(data)\n",
    "\n",
    "        if self.sort:\n",
    "            len_arr = np.array([d[\"text\"].shape[0] for d in data])\n",
    "            idx_arr = np.argsort(-len_arr)\n",
    "        else:\n",
    "            idx_arr = np.arange(data_size)\n",
    "\n",
    "        tail = idx_arr[len(idx_arr) - (len(idx_arr) % self.batch_size) :]\n",
    "        idx_arr = idx_arr[: len(idx_arr) - (len(idx_arr) % self.batch_size)]\n",
    "        idx_arr = idx_arr.reshape((-1, self.batch_size)).tolist()\n",
    "        if not self.drop_last and len(tail) > 0:\n",
    "            idx_arr += [tail.tolist()]\n",
    "\n",
    "        output = list()\n",
    "        for idx in idx_arr:\n",
    "            output.append(self.reprocess(data, idx))\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, cleaners):\n",
    "        self.cleaners = cleaners\n",
    "\n",
    "        self.basename, self.text, self.raw_text = self.process_meta(\n",
    "            filepath\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        basename = self.basename[idx]\n",
    "        raw_text = self.raw_text[idx]\n",
    "        phone = np.array(text_to_sequence(self.text[idx], self.cleaners))\n",
    "\n",
    "        return (basename, phone, raw_text)\n",
    "\n",
    "    def process_meta(self, filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            name = []\n",
    "            speaker = []\n",
    "            text = []\n",
    "            raw_text = []\n",
    "            for line in f.readlines():\n",
    "                n, s, t, r = line.strip(\"\\n\").split(\"|\")\n",
    "                name.append(n)\n",
    "                speaker.append(s)\n",
    "                text.append(t)\n",
    "                raw_text.append(r)\n",
    "            return name, speaker, text, raw_text\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        ids = [d[0] for d in data]\n",
    "        speakers = np.array([d[1] for d in data])\n",
    "        texts = [d[2] for d in data]\n",
    "        raw_texts = [d[3] for d in data]\n",
    "        text_lens = np.array([text.shape[0] for text in texts])\n",
    "\n",
    "        texts = pad_1D(texts)\n",
    "\n",
    "        return ids, raw_texts, speakers, texts, text_lens, max(text_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set  with size 300 is composed of 18 batches.\n",
      "Validation set  with size 100 is composed of 6 batches.\n"
     ]
    }
   ],
   "source": [
    "filename = \"metadata.csv\"  # El archivo que contiene los nombres de los archivos y sus metadatos\n",
    "dataset_name = \"F-VOICE\"\n",
    "preprocessed_path = \"C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\notebook_experimentos\\\\dereckpreprocessed\"\n",
    "cleaners = [\"spanish_cleaners\"]\n",
    "batch_size = 16\n",
    "trainset = \".\\\\dereckpreprocessed\\\\train.txt\"\n",
    "valset = \".\\\\dereckpreprocessed\\\\val.txt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "stats_path = \"C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\notebook_experimentos\\\\dereckpreprocessed\\\\stats.json\"\n",
    "graph_path =\"\\\\graficos\"\n",
    "\n",
    "# Crear una instancia del Dataset\n",
    "dataset = Dataset(filename, dataset_name, preprocessed_path, cleaners, batch_size)\n",
    "\n",
    "train_dataset = Dataset(\n",
    "        trainset, dataset_name,preprocessed_path,cleaners,batch_size=16, sort=True, drop_last=True)\n",
    "val_dataset = Dataset(\n",
    "        valset, dataset_name,preprocessed_path,cleaners,batch_size=16, sort=True, drop_last=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size * 4,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_dataset.collate_fn,\n",
    "    )\n",
    "val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=val_dataset.collate_fn,\n",
    "    )\n",
    "\n",
    "\n",
    "n_batch = 0\n",
    "for batchs in train_loader:\n",
    "    for batch in batchs:\n",
    "        to_device(batch, device)\n",
    "        n_batch += 1\n",
    "print(\"Training set  with size {} is composed of {} batches.\".format(len(train_dataset), n_batch ))\n",
    "\n",
    "n_batch = 0\n",
    "for batchs in val_loader:\n",
    "    for batch in batchs:\n",
    "        to_device(batch, device)\n",
    "        n_batch += 1\n",
    "print(\"Validation set  with size {} is composed of {} batches.\".format(len(val_dataset), n_batch))\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size * 4,\n",
    "        shuffle=True,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar modelo FastSpeech2 preentrenado\n",
    "model = FastSpeech2(preprocess_config, model_config).to(device)\n",
    "from model import FastSpeech2Loss\n",
    "restore_step = 100000 #Ultimo step del modeloe pre-entrenado\n",
    "ckpt_path = \"checkpoint\"\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "optimizer = ScheduledOptim(\n",
    "            model, train_config, model_config, restore_step\n",
    "        ).load_state_dict(ckpt[\"optimizer\"])\n",
    "model.train()\n",
    "model.eval()\n",
    "model.requires_grad_ = False\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "#freeze encoder parameters\n",
    "for param in model.module.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "#freeze pitch and energy embeddings\n",
    "model.module.variance_adaptor.pitch_embedding.weight.requires_grad = False\n",
    "model.module.variance_adaptor.energy_embedding.weight.requires_grad = False\n",
    "\n",
    "num_param = num_param = sum(param.numel() for param in model.parameters())\n",
    "Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "print(\"Number of FastSpeech2 Parameters:\", num_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, step,valname,dataset_name,train_config ,preprocess_config,model_config, logger=None, vocoder=None):\n",
    "\n",
    "    #preprocess_config, model_config = preprocessed_path\n",
    "\n",
    "    # Get dataset\n",
    "    dataset = Dataset(\n",
    "        valname, dataset_name, train_config, sort=False, drop_last=False\n",
    "    )\n",
    "    batch_size = train_config[\"optimizer\"][\"batch_size\"]\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "\n",
    "    # Get loss function\n",
    "    Loss = FastSpeech2Loss(preprocess_config, model_config).to(device)\n",
    "\n",
    "    # Evaluation\n",
    "    loss_sums = [0 for _ in range(6)]\n",
    "    for batchs in loader:\n",
    "        for batch in batchs:\n",
    "            batch = to_device(batch, device)\n",
    "            with torch.no_grad():\n",
    "                # Forward\n",
    "                output = model(*(batch[2:]))\n",
    "\n",
    "                # Cal Loss\n",
    "                losses = Loss(batch, output)\n",
    "\n",
    "                for i in range(len(losses)):\n",
    "                    loss_sums[i] += losses[i].item() * len(batch[0])\n",
    "\n",
    "    loss_means = [loss_sum / len(dataset) for loss_sum in loss_sums]\n",
    "\n",
    "    message = \"Validation Step {}, Total Loss: {:.4f}, Mel Loss: {:.4f}, Mel PostNet Loss: {:.4f}, Pitch Loss: {:.4f}, Energy Loss: {:.4f}, Duration Loss: {:.4f}\".format(\n",
    "        *([step] + [l for l in loss_means])\n",
    "    )\n",
    "\n",
    "    if logger is not None:\n",
    "        fig, wav_reconstruction, wav_prediction, tag = synth_one_sample(\n",
    "            batch,\n",
    "            output,\n",
    "            vocoder,\n",
    "            model_config,\n",
    "            preprocess_config,\n",
    "        )\n",
    "\n",
    "        log(logger, step, losses=loss_means)\n",
    "        log(\n",
    "            logger,\n",
    "            fig=fig,\n",
    "            tag=\"Validation/step_{}_{}\".format(step, tag),\n",
    "        )\n",
    "        sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "        log(\n",
    "            logger,\n",
    "            audio=wav_reconstruction,\n",
    "            sampling_rate=sampling_rate,\n",
    "            tag=\"Validation/step_{}_{}_reconstructed\".format(step, tag),\n",
    "        )\n",
    "        log(\n",
    "            logger,\n",
    "            audio=wav_prediction,\n",
    "            sampling_rate=sampling_rate,\n",
    "            tag=\"Validation/step_{}_{}_synthesized\".format(step, tag),\n",
    "        )\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOCODER \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se debe cargar el modelo preentrado de hifi-gan\n",
    "import json\n",
    "with open(\".\\\\hifigan\\\\config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        config = hifigan.AttrDict(config)\n",
    "        vocoder = hifigan.Generator(config)\n",
    "ckpt = torch.load(\"hifigan/generator_universal.pth.tar\")\n",
    "vocoder.load_state_dict(ckpt[\"generator\"])\n",
    "vocoder.eval()\n",
    "vocoder.remove_weight_norm()\n",
    "vocoder.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "train_log_path = \"\"\n",
    "val_log_path = \"\"\n",
    "os.makedirs(train_log_path, exist_ok=True)\n",
    "os.makedirs(val_log_path, exist_ok=True)\n",
    "train_logger = SummaryWriter(train_log_path)\n",
    "val_logger = SummaryWriter(val_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = restore_step + 1\n",
    "epoch = 1\n",
    "grad_acc_step = train_config[\"optimizer\"][\"grad_acc_step\"]\n",
    "grad_clip_thresh = train_config[\"optimizer\"][\"grad_clip_thresh\"]\n",
    "total_step = train_config[\"step\"][\"total_step\"]\n",
    "log_step = train_config[\"step\"][\"log_step\"]\n",
    "save_step = train_config[\"step\"][\"save_step\"]\n",
    "synth_step = train_config[\"step\"][\"synth_step\"]\n",
    "val_step = train_config[\"step\"][\"val_step\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "outer_bar = tqdm(total=total_step, desc=\"Training\", position=0)\n",
    "outer_bar.n = restore_step\n",
    "outer_bar.update()\n",
    "\n",
    "while True:\n",
    "    inner_bar = tqdm(total=len(loader), desc=\"Epoch {}\".format(epoch), position=1)\n",
    "    for batchs in loader:\n",
    "        for batch in batchs:\n",
    "            batch = to_device(batch, device)\n",
    "\n",
    "            # Forward\n",
    "            output = model(*(batch[2:]))\n",
    "\n",
    "            # Calculate Loss\n",
    "            losses = Loss(batch, output)\n",
    "            total_loss = losses[0]\n",
    "\n",
    "            # Backward\n",
    "            total_loss = total_loss / grad_acc_step\n",
    "            total_loss.backward()\n",
    "            if step % grad_acc_step == 0:\n",
    "                # Clipping gradients to avoid gradient explosion\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step_and_update_lr()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if step % log_step == 0:\n",
    "                losses = [l.item() for l in losses]\n",
    "                message1 = \"Step {}/{}, \".format(step, total_step)\n",
    "                message2 = \"Total Loss: {:.4f}, Mel Loss: {:.4f}, Mel PostNet Loss: {:.4f}, Pitch Loss: {:.4f}, Energy Loss: {:.4f}, Duration Loss: {:.4f}\".format(\n",
    "                    *losses\n",
    "                )\n",
    "\n",
    "                with open(os.path.join(train_log_path, \"log.txt\"), \"a\") as f:\n",
    "                    f.write(message1 + message2 + \"\\n\")\n",
    "\n",
    "                outer_bar.write(message1 + message2)\n",
    "\n",
    "                log(train_logger, step, losses=losses)\n",
    "\n",
    "            if step % synth_step == 0:\n",
    "                fig, wav_reconstruction, wav_prediction, tag = synth_one_sample(\n",
    "                    batch,\n",
    "                    output,\n",
    "                    vocoder,\n",
    "                    model_config = model_config,\n",
    "                    preprocess_config=preprocess_config,\n",
    "                )\n",
    "\n",
    "                log(\n",
    "                    train_logger,\n",
    "                    fig=fig,\n",
    "                    tag=\"Training/step_{}_{}\".format(step, tag),\n",
    "                )\n",
    "                sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\n",
    "                    \"sampling_rate\"\n",
    "                ]\n",
    "                log(\n",
    "                    train_logger,\n",
    "                    audio=wav_reconstruction,\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    tag=\"Training/step_{}_{}_reconstructed\".format(step, tag),\n",
    "                )\n",
    "                log(\n",
    "                    train_logger,\n",
    "                    audio=wav_prediction,\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    tag=\"Training/step_{}_{}_synthesized\".format(step, tag),\n",
    "                )\n",
    "\n",
    "            if step % val_step == 0:\n",
    "                model.eval()\n",
    "                message = evaluate(model, step,valset,dataset_name,train_config ,preprocess_config,model_config, val_logger, vocoder)\n",
    "                with open(os.path.join(val_log_path, \"log.txt\"), \"a\") as f:\n",
    "                    f.write(message + \"\\n\")\n",
    "                outer_bar.write(message)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if step % save_step == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer\": optimizer._optimizer.state_dict(),\n",
    "                    },\n",
    "                    os.path.join(\n",
    "                       ckpt_path,\n",
    "                        \"{}.pth.tar\".format(step),\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            if step == total_step:\n",
    "                quit()\n",
    "            step += 1\n",
    "            outer_bar.update(1)\n",
    "\n",
    "        inner_bar.update(1)\n",
    "    epoch += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
