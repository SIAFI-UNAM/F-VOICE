{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import voice_to_text as vtt\n",
    "import Mel_creator as mc\n",
    "\n",
    "from data_utils_2 import TextMelLoader, TextMelCollate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from logger import Tacotron2Logger\n",
    "from distributed import apply_gradient_allreduce\n",
    "from loss_function import Tacotron2Loss\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "from numpy import finfo\n",
    "import os\n",
    "import argparse\n",
    "from model_FV import Tacotron2\n",
    "import tqdm\n",
    "from shutil import copytree\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(hparams):\n",
    "    # Get data, data loaders and collate function ready\n",
    "    trainset = TextMelLoader(hparams['training_files'], hparams['text_cleaners'],\n",
    "                         hparams['max_wav_value'], hparams['sampling_rate'],\n",
    "                         hparams['load_mel_from_disk'], hparams['filter_length'],\n",
    "                         hparams['hop_length'], hparams['win_length'],\n",
    "                         hparams['n_mel_channels'], hparams['mel_fmin'],\n",
    "                         hparams['mel_fmax'], hparams['seed'])\n",
    "    valset = TextMelLoader( hparams['validation_files'], hparams['text_cleaners'],\n",
    "                         hparams['max_wav_value'], hparams['sampling_rate'],\n",
    "                         hparams['load_mel_from_disk'], hparams['filter_length'],\n",
    "                         hparams['hop_length'], hparams['win_length'],\n",
    "                         hparams['n_mel_channels'], hparams['mel_fmin'],\n",
    "                         hparams['mel_fmax'], hparams['seed'])\n",
    "    collate_fn = TextMelCollate(n_frames_per_step=1)\n",
    "\n",
    "    if hparams['distributed_run']:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=hparams['batch_size'], pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt\n",
    "\n",
    "def init_distributed(n_gpus, rank, group_name):\n",
    "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
    "    print(\"Initializing Distributed\")\n",
    "\n",
    "    # Set cuda device so everything is done on the right GPU.\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "    # Initialize distributed communication\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\", init_method=\"tcp://localhost:54321\",\n",
    "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
    "    \n",
    "    print(\"Done initializing distributed\")\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for line in data:\n",
    "            file.write(' | '.join(line) + '\\n')\n",
    "\n",
    "def plot_alignment(alignment, info=None):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "##Funciones en construccion\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2().cuda()  ##Josue\n",
    "    if hparams['fp16_run']:\n",
    "        model.decoder.attention_layer.score_mask_value = ('float16').min\n",
    "\n",
    "    if hparams['distributed_run']:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
    "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
    "        iteration, filepath))\n",
    "    try:\n",
    "        torch.save({'iteration': iteration,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt received while saving, waiting for save to complete.\")\n",
    "        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "##Validation process\n",
    "\n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate,sample_interbal, save_audio = False, audio_path = None):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
    "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            if distributed_run:\n",
    "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "        _, mel_outputs, gate_outputs, alignments = y_pred\n",
    "        idx = random.randint(0, alignments.size(0) - 1)\n",
    "        plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "##Training process\n",
    "\n",
    "def train( log_directory, checkpoint_path, warm_start, n_gpus,\n",
    "          rank, group_name, hparams, log_directory2):\n",
    "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    log_directory (string) directory to save tensorboard logs\n",
    "    checkpoint_path(string): checkpoint path\n",
    "    n_gpus (int): number of gpus\n",
    "    rank (int): rank of current gpu\n",
    "    hparams (object): comma separated list of \"name=value\" pairs.\n",
    "    \"\"\"\n",
    "    if hparams['distributed_run']:\n",
    "        init_distributed(n_gpus, rank, group_name)\n",
    "\n",
    "    torch.manual_seed(hparams['seed'])\n",
    "    torch.cuda.manual_seed(hparams['seed'])\n",
    "\n",
    "    model = load_model(hparams)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'],\n",
    "                                 weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    if hparams['fp16_run']:\n",
    "        scaler = GradScaler()\n",
    "    if hparams['distributed_run']:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    logger = prepare_directories_and_logger(\n",
    "        hparams['ouputh_checkpoint_path'], hparams['log_directory_1'], hparams['rank'])\n",
    "\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "\n",
    "    # Load checkpoint if one exists\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams['ignore_layers'])\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams['use_saved_learning_rate']:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1  # next iteration is iteration + 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "    else:\n",
    "      model = warm_start_model(hparams['ouputh_checkpoint_path'], model, hparams['ignore_layers'])\n",
    "      # download LJSpeech pretrained model if no checkpoint already exists\n",
    "    \n",
    "    start_eposh = time.perf_counter()\n",
    "    learning_rate = 0.0\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in tqdm(range(epoch_offset, hparams['epochs'])):\n",
    "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
    "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            start = time.perf_counter()\n",
    "            if iteration < hparams['decay_start']: learning_rate = 5e-4\n",
    "            else: iteration_adjusted = iteration - hparams['decay_start']; learning_rate = (5e-4*(e**(-iteration_adjusted/8000))) + 0\n",
    "            learning_rate = max(hparams['min_learning_rate'] , learning_rate) # output the largest number\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            with autocast(enabled= hparams['fp16_run']):\n",
    "                y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            if hparams['distributed_run']:\n",
    "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            if  hparams['fp16_run']:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if  hparams['fp16_run']:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1.0)\n",
    "                is_overflow = math.isnan(grad_norm)\n",
    "                if not is_overflow:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "                print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
    "\n",
    "            iteration += 1\n",
    "        validate(model, criterion, valset, iteration,\n",
    "                 hparams['batch_size'], n_gpus, collate_fn, logger,\n",
    "                 hparams['distributed_run'], rank, epoch, start_eposh, learning_rate)\n",
    "        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
    "        if log_directory2 is not None:\n",
    "            copytree(log_directory, log_directory2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'in_audio_path':\"E:\\\\wavs\",\n",
    "    'out_audio_path':\"E:\\\\wavs_text\",\n",
    "    'whisper_model':'base',\n",
    "    'whipser_language':'english',\n",
    "    'out_audio_mels_path':\"E:\\\\mels\",\n",
    "    'ouputh_checkpoint_path':'E:\\\\CH_output\\\\pretrained_model.pt',\n",
    "    'log_directory_1':\"E:\\\\loggs_1\",\n",
    "    'log_directory_2':'E:\\\\loggs_2',\n",
    "    ################################\n",
    "    # Data Parameters              #\n",
    "    ################################\n",
    "    'training_files':'E:\\\\wavs_text\\\\train.txt',\n",
    "    'validation_files':'E:\\\\wavs_text\\\\val.txt',\n",
    "    'n_gpus':1,\n",
    "    'rank':1,\n",
    "    'group_name':None,\n",
    "    'text_cleaners': ['english_cleaners'],\n",
    "    ################################\n",
    "    # Audio Parameters             #\n",
    "    ################################\n",
    "    'max_wav_value': 32768.0,\n",
    "    'sampling_rate': 22050,\n",
    "    'load_mel_from_disk': False,\n",
    "    'filter_length': 1024,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 1024,\n",
    "    'n_mel_channels': 80,\n",
    "    'mel_fmin': 0.0,\n",
    "    'mel_fmax': 8000.0,\n",
    "    'seed': 20,\n",
    "     ################################\n",
    "     # Optimization Hyperparameters #\n",
    "    ################################\n",
    "    'use_saved_learning_rate':False,\n",
    "    'learning_rate':1e-3,\n",
    "    'weight_decay':1e-6,\n",
    "    'grad_clip_thresh':1.0,\n",
    "    'batch_size':64,\n",
    "    'mask_padding':True,# set model's padded outputs to padded values\n",
    "    ################################\n",
    "    # Experiment Parameters        #\n",
    "    ################################\n",
    "    'epochs':500,\n",
    "    'iters_per_checkpoint':1000,\n",
    "    'dynamic_loss_scaling':True,\n",
    "    'fp16_run':False,\n",
    "    'distributed_run':False,\n",
    "    'dist_backend':\"gloo\",\n",
    "    'dist_url':\"tcp://localhost:54321\",\n",
    "    'cudnn_enabled':True,\n",
    "    'cudnn_benchmark':True,\n",
    "    'ignore_layers':['embedding.weight'],\n",
    "    'decay_start': 15000,\n",
    "    'min_learning_rate': 1e-5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 create Text from audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13100/13100 [1:12:05<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "vtt.voice_to_text(hparams['in_audio_path'],hparams['out_audio_path'],language=\"english\") #Here creates the audio files to a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Audio To mels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mels state: 100%|██████████| 13100/13100 [3:26:20<00:00,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavs_mel created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mc.mel_creator(hparams['in_audio_path'],hparams['out_audio_mels_path']) #Here creates the audio files to Mel tensor's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hparams['ouputh_checkpoint_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete training and validation datasets\n",
    "data = []\n",
    "with open('E:\\\\wavs_text\\\\wavs_text.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        data.append(line.strip().split(' | '))\n",
    "import random\n",
    "\n",
    "random.shuffle(data)  # Mezclar los datos aleatoriamente\n",
    "\n",
    "# Calcular los tamaños de los conjuntos de datos\n",
    "total_samples = len(data)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.30 * total_samples)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "\n",
    "\n",
    "\n",
    "save_data(train_data, 'E:\\\\wavs_text\\\\train.txt')\n",
    "save_data(val_data, 'E:\\\\wavs_text\\\\val.txt')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm starting model from checkpoint 'E:\\CH_output\\pretrained_model.pt'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"encoder.convolutions.1.0.conv.weight\", \"encoder.convolutions.1.0.conv.bias\", \"encoder.convolutions.1.1.weight\", \"encoder.convolutions.1.1.bias\", \"encoder.convolutions.1.1.running_mean\", \"encoder.convolutions.1.1.running_var\", \"encoder.convolutions.1.1.num_batches_tracked\", \"encoder.convolutions.2.0.conv.weight\", \"encoder.convolutions.2.0.conv.bias\", \"encoder.convolutions.2.1.weight\", \"encoder.convolutions.2.1.bias\", \"encoder.convolutions.2.1.running_mean\", \"encoder.convolutions.2.1.running_var\", \"encoder.convolutions.2.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_directory_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m       \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mn_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_gpus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgroup_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m       \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_directory2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_directory_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 233\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(log_directory, checkpoint_path, warm_start, n_gpus, rank, group_name, hparams, log_directory2)\u001b[0m\n\u001b[0;32m    231\u001b[0m         epoch_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(iteration \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)))\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mwarm_start_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouputh_checkpoint_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m   \u001b[38;5;66;03m# download LJSpeech pretrained model if no checkpoint already exists\u001b[39;00m\n\u001b[0;32m    236\u001b[0m start_eposh \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "Cell \u001b[1;32mIn[25], line 109\u001b[0m, in \u001b[0;36mwarm_start_model\u001b[1;34m(checkpoint_path, model, ignore_layers)\u001b[0m\n\u001b[0;32m    107\u001b[0m     dummy_dict\u001b[38;5;241m.\u001b[39mupdate(model_dict)\n\u001b[0;32m    108\u001b[0m     model_dict \u001b[38;5;241m=\u001b[39m dummy_dict\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"encoder.convolutions.1.0.conv.weight\", \"encoder.convolutions.1.0.conv.bias\", \"encoder.convolutions.1.1.weight\", \"encoder.convolutions.1.1.bias\", \"encoder.convolutions.1.1.running_mean\", \"encoder.convolutions.1.1.running_var\", \"encoder.convolutions.1.1.num_batches_tracked\", \"encoder.convolutions.2.0.conv.weight\", \"encoder.convolutions.2.0.conv.bias\", \"encoder.convolutions.2.1.weight\", \"encoder.convolutions.2.1.bias\", \"encoder.convolutions.2.1.running_mean\", \"encoder.convolutions.2.1.running_var\", \"encoder.convolutions.2.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "train(log_directory=hparams['log_directory_1'], checkpoint_path=checkpoint_path,\n",
    "       warm_start=True,n_gpus=hparams['n_gpus'], rank=hparams['rank'],group_name= hparams['group_name'], \n",
    "       hparams=hparams, log_directory2=hparams['log_directory_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(hparams):\n",
    "    model = Tacotron2().cuda()  ##Josue\n",
    "    if hparams['fp16_run']:\n",
    "        model.decoder.attention_layer.score_mask_value = ('float16').min\n",
    "\n",
    "    if hparams['distributed_run']:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm starting model from checkpoint 'E:\\CH_output\\pretrained_model.pt'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"encoder.convolutions.1.0.conv.weight\", \"encoder.convolutions.1.0.conv.bias\", \"encoder.convolutions.1.1.weight\", \"encoder.convolutions.1.1.bias\", \"encoder.convolutions.1.1.running_mean\", \"encoder.convolutions.1.1.running_var\", \"encoder.convolutions.1.1.num_batches_tracked\", \"encoder.convolutions.2.0.conv.weight\", \"encoder.convolutions.2.0.conv.bias\", \"encoder.convolutions.2.1.weight\", \"encoder.convolutions.2.1.bias\", \"encoder.convolutions.2.1.running_mean\", \"encoder.convolutions.2.1.running_var\", \"encoder.convolutions.2.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwarm_start_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouputh_checkpoint_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 109\u001b[0m, in \u001b[0;36mwarm_start_model\u001b[1;34m(checkpoint_path, model, ignore_layers)\u001b[0m\n\u001b[0;32m    107\u001b[0m     dummy_dict\u001b[38;5;241m.\u001b[39mupdate(model_dict)\n\u001b[0;32m    108\u001b[0m     model_dict \u001b[38;5;241m=\u001b[39m dummy_dict\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Tacotron2:\n\tUnexpected key(s) in state_dict: \"encoder.convolutions.1.0.conv.weight\", \"encoder.convolutions.1.0.conv.bias\", \"encoder.convolutions.1.1.weight\", \"encoder.convolutions.1.1.bias\", \"encoder.convolutions.1.1.running_mean\", \"encoder.convolutions.1.1.running_var\", \"encoder.convolutions.1.1.num_batches_tracked\", \"encoder.convolutions.2.0.conv.weight\", \"encoder.convolutions.2.0.conv.bias\", \"encoder.convolutions.2.1.weight\", \"encoder.convolutions.2.1.bias\", \"encoder.convolutions.2.1.running_mean\", \"encoder.convolutions.2.1.running_var\", \"encoder.convolutions.2.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "warm_start_model(hparams['ouputh_checkpoint_path'], load_model(hparams), hparams['ignore_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tacotron2(\n",
       "  (embedding): Embedding(256, 512)\n",
       "  (encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (layers): ModuleList(\n",
       "        (0): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(768, 1024)\n",
       "    (attention_layer): Attention(\n",
       "      (query_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (memory_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv): ConvNorm(\n",
       "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        )\n",
       "        (location_dense): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\n",
       "    (linear_projection): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)\n",
       "    )\n",
       "    (gate_layer): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (postnet): Postnet(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Tacotron2:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"encoder.convolutions.0.0.conv.weight\", \"encoder.convolutions.0.0.conv.bias\", \"encoder.convolutions.0.1.weight\", \"encoder.convolutions.0.1.bias\", \"encoder.convolutions.0.1.running_mean\", \"encoder.convolutions.0.1.running_var\", \"encoder.lstm.weight_ih_l0\", \"encoder.lstm.weight_hh_l0\", \"encoder.lstm.bias_ih_l0\", \"encoder.lstm.bias_hh_l0\", \"encoder.lstm.weight_ih_l0_reverse\", \"encoder.lstm.weight_hh_l0_reverse\", \"encoder.lstm.bias_ih_l0_reverse\", \"encoder.lstm.bias_hh_l0_reverse\", \"decoder.prenet.layers.0.linear_layer.weight\", \"decoder.prenet.layers.1.linear_layer.weight\", \"decoder.attention_rnn.weight_ih\", \"decoder.attention_rnn.weight_hh\", \"decoder.attention_rnn.bias_ih\", \"decoder.attention_rnn.bias_hh\", \"decoder.attention_layer.query_layer.linear_layer.weight\", \"decoder.attention_layer.memory_layer.linear_layer.weight\", \"decoder.attention_layer.v.linear_layer.weight\", \"decoder.attention_layer.location_layer.location_conv.conv.weight\", \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\", \"decoder.decoder_rnn.weight_ih\", \"decoder.decoder_rnn.weight_hh\", \"decoder.decoder_rnn.bias_ih\", \"decoder.decoder_rnn.bias_hh\", \"decoder.linear_projection.linear_layer.weight\", \"decoder.linear_projection.linear_layer.bias\", \"decoder.gate_layer.linear_layer.weight\", \"decoder.gate_layer.linear_layer.bias\", \"postnet.convolutions.0.0.conv.weight\", \"postnet.convolutions.0.0.conv.bias\", \"postnet.convolutions.0.1.weight\", \"postnet.convolutions.0.1.bias\", \"postnet.convolutions.0.1.running_mean\", \"postnet.convolutions.0.1.running_var\", \"postnet.convolutions.1.0.conv.weight\", \"postnet.convolutions.1.0.conv.bias\", \"postnet.convolutions.1.1.weight\", \"postnet.convolutions.1.1.bias\", \"postnet.convolutions.1.1.running_mean\", \"postnet.convolutions.1.1.running_var\", \"postnet.convolutions.2.0.conv.weight\", \"postnet.convolutions.2.0.conv.bias\", \"postnet.convolutions.2.1.weight\", \"postnet.convolutions.2.1.bias\", \"postnet.convolutions.2.1.running_mean\", \"postnet.convolutions.2.1.running_var\", \"postnet.convolutions.3.0.conv.weight\", \"postnet.convolutions.3.0.conv.bias\", \"postnet.convolutions.3.1.weight\", \"postnet.convolutions.3.1.bias\", \"postnet.convolutions.3.1.running_mean\", \"postnet.convolutions.3.1.running_var\", \"postnet.convolutions.4.0.conv.weight\", \"postnet.convolutions.4.0.conv.bias\", \"postnet.convolutions.4.1.weight\", \"postnet.convolutions.4.1.bias\", \"postnet.convolutions.4.1.running_mean\", \"postnet.convolutions.4.1.running_var\". \n\tUnexpected key(s) in state_dict: \"state_dict\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouputh_checkpoint_path\u001b[39m\u001b[38;5;124m'\u001b[39m], map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Establece el modo de evaluación\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Tacotron2:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"encoder.convolutions.0.0.conv.weight\", \"encoder.convolutions.0.0.conv.bias\", \"encoder.convolutions.0.1.weight\", \"encoder.convolutions.0.1.bias\", \"encoder.convolutions.0.1.running_mean\", \"encoder.convolutions.0.1.running_var\", \"encoder.lstm.weight_ih_l0\", \"encoder.lstm.weight_hh_l0\", \"encoder.lstm.bias_ih_l0\", \"encoder.lstm.bias_hh_l0\", \"encoder.lstm.weight_ih_l0_reverse\", \"encoder.lstm.weight_hh_l0_reverse\", \"encoder.lstm.bias_ih_l0_reverse\", \"encoder.lstm.bias_hh_l0_reverse\", \"decoder.prenet.layers.0.linear_layer.weight\", \"decoder.prenet.layers.1.linear_layer.weight\", \"decoder.attention_rnn.weight_ih\", \"decoder.attention_rnn.weight_hh\", \"decoder.attention_rnn.bias_ih\", \"decoder.attention_rnn.bias_hh\", \"decoder.attention_layer.query_layer.linear_layer.weight\", \"decoder.attention_layer.memory_layer.linear_layer.weight\", \"decoder.attention_layer.v.linear_layer.weight\", \"decoder.attention_layer.location_layer.location_conv.conv.weight\", \"decoder.attention_layer.location_layer.location_dense.linear_layer.weight\", \"decoder.decoder_rnn.weight_ih\", \"decoder.decoder_rnn.weight_hh\", \"decoder.decoder_rnn.bias_ih\", \"decoder.decoder_rnn.bias_hh\", \"decoder.linear_projection.linear_layer.weight\", \"decoder.linear_projection.linear_layer.bias\", \"decoder.gate_layer.linear_layer.weight\", \"decoder.gate_layer.linear_layer.bias\", \"postnet.convolutions.0.0.conv.weight\", \"postnet.convolutions.0.0.conv.bias\", \"postnet.convolutions.0.1.weight\", \"postnet.convolutions.0.1.bias\", \"postnet.convolutions.0.1.running_mean\", \"postnet.convolutions.0.1.running_var\", \"postnet.convolutions.1.0.conv.weight\", \"postnet.convolutions.1.0.conv.bias\", \"postnet.convolutions.1.1.weight\", \"postnet.convolutions.1.1.bias\", \"postnet.convolutions.1.1.running_mean\", \"postnet.convolutions.1.1.running_var\", \"postnet.convolutions.2.0.conv.weight\", \"postnet.convolutions.2.0.conv.bias\", \"postnet.convolutions.2.1.weight\", \"postnet.convolutions.2.1.bias\", \"postnet.convolutions.2.1.running_mean\", \"postnet.convolutions.2.1.running_var\", \"postnet.convolutions.3.0.conv.weight\", \"postnet.convolutions.3.0.conv.bias\", \"postnet.convolutions.3.1.weight\", \"postnet.convolutions.3.1.bias\", \"postnet.convolutions.3.1.running_mean\", \"postnet.convolutions.3.1.running_var\", \"postnet.convolutions.4.0.conv.weight\", \"postnet.convolutions.4.0.conv.bias\", \"postnet.convolutions.4.1.weight\", \"postnet.convolutions.4.1.bias\", \"postnet.convolutions.4.1.running_mean\", \"postnet.convolutions.4.1.running_var\". \n\tUnexpected key(s) in state_dict: \"state_dict\". "
     ]
    }
   ],
   "source": [
    "model_dict = torch.load(hparams['ouputh_checkpoint_path'], map_location='cpu')\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# Establece el modo de evaluación\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAVEGLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LJ001-0001.npy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_WAV_VALUE = 32768.0\n",
    "mel_files = os.listdir(\"E:\\\\Github\\\\F-voice\\\\F-VOICE\\\\train_model_0\\\\waveglow\\\\mel_spectograms\")\n",
    "mel_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from denoiser import Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def main(mels_files, waveglow_path, sigma, output_dir, sampling_rate, is_fp16, denoiser_strength):\n",
    "    mels = os.listdir(mel_files)\n",
    "    waveglow = torch.load(waveglow_path)['model']\n",
    "    waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "    waveglow = waveglow.cuda().eval()\n",
    "\n",
    "    if denoiser_strength > 0:\n",
    "        denoiser = Denoiser(waveglow).cuda()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()  # Create a GradScaler for mixed precision\n",
    "\n",
    "    for i, file_path in enumerate(os.listdir(mels_files)):\n",
    "        file_name = os.listdir(mel_files)[0]\n",
    "        mel = np.load(mel_files+\"\\\\\"+file_name)  # Load the .npy file as a NumPy array\n",
    "        mel = torch.FloatTensor(mel).cuda()  # Convert the NumPy array to a PyTorch tensor and move it to GPU\n",
    "        mel = torch.unsqueeze(mel, 0)\n",
    "        mel = mel.half() if is_fp16 else mel\n",
    "        print(mel.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use torch.cuda.amp.autocast for mixed precision\n",
    "            if is_fp16:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    audio = waveglow.infer(mel, sigma=sigma)\n",
    "                    if denoiser_strength > 0:\n",
    "                        audio = denoiser(audio, denoiser_strength)\n",
    "                    audio = audio * MAX_WAV_VALUE\n",
    "            else:\n",
    "                audio = waveglow.infer(mel, sigma=sigma)\n",
    "                if denoiser_strength > 0:\n",
    "                    audio = denoiser(audio, denoiser_strength)\n",
    "                audio = audio * MAX_WAV_VALUE\n",
    "\n",
    "        audio = audio.squeeze()\n",
    "        audio = audio.cpu().numpy()\n",
    "        audio = audio.astype('int16')\n",
    "        audio_path = os.path.join(output_dir, \"{}_synthesis.wav\".format(file_name))\n",
    "        write(audio_path, sampling_rate, audio)\n",
    "        print(audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.conv.ConvTranspose1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "c:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "c:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1113: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 513, 832])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [80, 80, 1024], expected input[1, 513, 832] to have 80 channels, but got 513 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmels_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-voice\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-VOICE\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain_model_0\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mwaveglow\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmel_spectograms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaveglow_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-voice\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-VOICE\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain_model_0\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mwaveglow_256channels.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGithub\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-voice\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mF-VOICE\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain_model_0\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mwaveglow\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmel_spectograms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22050\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_fp16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoiser_strength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(mels_files, waveglow_path, sigma, output_dir, sampling_rate, is_fp16, denoiser_strength)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fp16:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 25\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mwaveglow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m denoiser_strength \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m             audio \u001b[38;5;241m=\u001b[39m denoiser(audio, denoiser_strength)\n",
      "File \u001b[1;32me:\\Github\\F-voice\\F-VOICE\\train_model_0\\glow.py:252\u001b[0m, in \u001b[0;36mWaveGlow.infer\u001b[1;34m(self, spect, sigma)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\u001b[38;5;28mself\u001b[39m, spect, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m--> 252\u001b[0m     spect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# trim conv artifacts. maybe pad spec to kernel multiple\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     time_cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample\u001b[38;5;241m.\u001b[39mkernel_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Laptop Bonita\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:797\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    793\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    794\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    795\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    796\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [80, 80, 1024], expected input[1, 513, 832] to have 80 channels, but got 513 channels instead"
     ]
    }
   ],
   "source": [
    "main(mels_files=\"E:\\\\Github\\\\F-voice\\\\F-VOICE\\\\train_model_0\\\\waveglow\\\\mel_spectograms\", waveglow_path=\"E:\\\\Github\\\\F-voice\\\\F-VOICE\\\\train_model_0\\\\waveglow_256channels.pt\", sigma=0.6,output_dir=\"E:\\\\Github\\\\F-voice\\\\F-VOICE\\\\train_model_0\\\\waveglow\\\\mel_spectograms\" ,\n",
    "         sampling_rate=22050, is_fp16 = True, denoiser_strength = 0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
