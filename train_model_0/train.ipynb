{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import voice_to_text as vtt\n",
    "import Mel_creator as mc\n",
    "\n",
    "from data_utils_2 import TextMelLoader, TextMelCollate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from logger import Tacotron2Logger\n",
    "from distributed import apply_gradient_allreduce\n",
    "from loss_function import Tacotron2Loss\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "from numpy import finfo\n",
    "import os\n",
    "import argparse\n",
    "from model_FV import Tacotron2\n",
    "import tqdm\n",
    "from shutil import copytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(hparams):\n",
    "    # Get data, data loaders and collate function ready\n",
    "    trainset = TextMelLoader(hparams['training_files'], hparams['text_cleaners'],\n",
    "                         hparams['max_wav_value'], hparams['sampling_rate'],\n",
    "                         hparams['load_mel_from_disk'], hparams['filter_length'],\n",
    "                         hparams['hop_length'], hparams['win_length'],\n",
    "                         hparams['n_mel_channels'], hparams['mel_fmin'],\n",
    "                         hparams['mel_fmax'], hparams['seed'])\n",
    "    valset = TextMelLoader( hparams['validation_files'], hparams['text_cleaners'],\n",
    "                         hparams['max_wav_value'], hparams['sampling_rate'],\n",
    "                         hparams['load_mel_from_disk'], hparams['filter_length'],\n",
    "                         hparams['hop_length'], hparams['win_length'],\n",
    "                         hparams['n_mel_channels'], hparams['mel_fmin'],\n",
    "                         hparams['mel_fmax'], hparams['seed'])\n",
    "    collate_fn = TextMelCollate(n_frames_per_step=1)\n",
    "\n",
    "    if hparams['distributed_run']:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=hparams['batch_size'], pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt\n",
    "\n",
    "def init_distributed(n_gpus, rank, group_name):\n",
    "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
    "    print(\"Initializing Distributed\")\n",
    "\n",
    "    # Set cuda device so everything is done on the right GPU.\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "    # Initialize distributed communication\n",
    "    dist.init_process_group(\n",
    "        backend=\"gloo\", init_method=\"tcp://localhost:54321\",\n",
    "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
    "    \n",
    "    print(\"Done initializing distributed\")\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "\n",
    "##Funciones en construccion\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2().cuda()  ##Josue\n",
    "    if hparams['fp16_run']:\n",
    "        model.decoder.attention_layer.score_mask_value = ('float16').min\n",
    "\n",
    "    if hparams['distributed_run']:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
    "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
    "        iteration, filepath))\n",
    "    try:\n",
    "        torch.save({'iteration': iteration,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"interrupt received while saving, waiting for save to complete.\")\n",
    "        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "##Validation process\n",
    "\n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
    "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            if distributed_run:\n",
    "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "       \n",
    "##Training process\n",
    "\n",
    "def train( log_directory, checkpoint_path, warm_start, n_gpus,\n",
    "          rank, group_name, hparams, log_directory2):\n",
    "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    log_directory (string) directory to save tensorboard logs\n",
    "    checkpoint_path(string): checkpoint path\n",
    "    n_gpus (int): number of gpus\n",
    "    rank (int): rank of current gpu\n",
    "    hparams (object): comma separated list of \"name=value\" pairs.\n",
    "    \"\"\"\n",
    "    if hparams['distributed_run']:\n",
    "        init_distributed(n_gpus, rank, group_name)\n",
    "\n",
    "    torch.manual_seed(hparams['seed'])\n",
    "    torch.cuda.manual_seed(hparams['seed'])\n",
    "\n",
    "    model = load_model(hparams)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'],\n",
    "                                 weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    if hparams['fp16_run']:\n",
    "        scaler = GradScaler()\n",
    "    if hparams['distributed_run']:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    logger = prepare_directories_and_logger(\n",
    "        hparams['ouputh_checkpoint_path'], hparams['log_directory_1'], hparams['rank'])\n",
    "\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "\n",
    "    # Load checkpoint if one exists\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams['ignore_layers'])\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams['use_saved_learning_rate']:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1  # next iteration is iteration + 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "    else:\n",
    "      model = warm_start_model(\"pretrained_model\", model, hparams['ignore_layers'])\n",
    "      # download LJSpeech pretrained model if no checkpoint already exists\n",
    "    \n",
    "    start_eposh = time.perf_counter()\n",
    "    learning_rate = 0.0\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in tqdm(range(epoch_offset, hparams['epochs'])):\n",
    "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
    "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            start = time.perf_counter()\n",
    "            if iteration < hparams['decay_start']: learning_rate = 5e-4\n",
    "            else: iteration_adjusted = iteration - hparams['decay_start']; learning_rate = (5e-4*(e**(-iteration_adjusted/8000))) + 0\n",
    "            learning_rate = max(hparams['min_learning_rate'] , learning_rate) # output the largest number\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            with autocast(enabled= hparams['fp16_run']):\n",
    "                y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            if hparams['distributed_run']:\n",
    "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            if  hparams['fp16_run']:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if  hparams['fp16_run']:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1.0)\n",
    "                is_overflow = math.isnan(grad_norm)\n",
    "                if not is_overflow:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "                print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
    "\n",
    "            iteration += 1\n",
    "        validate(model, criterion, valset, iteration,\n",
    "                 hparams['batch_size'], n_gpus, collate_fn, logger,\n",
    "                 hparams['distributed_run'], rank, epoch, start_eposh, learning_rate)\n",
    "        save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
    "        if log_directory2 is not None:\n",
    "            copytree(log_directory, log_directory2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'in_audio_path':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\audio_test',\n",
    "    'out_audio_path':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE',\n",
    "    'whisper_model':'base',\n",
    "    'whipser_language':'english',\n",
    "    'out_audio_mels_path':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\mels',\n",
    "    'ouputh_checkpoint_path':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\checkpoints',\n",
    "    'log_directory_1':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\logs1',\n",
    "    'log_directory_2':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\logs2',\n",
    "    ################################\n",
    "    # Data Parameters              #\n",
    "    ################################\n",
    "    'training_files':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\wavs_text.txt',\n",
    "    'validation_files':'C:\\\\Users\\\\derec\\\\OneDrive\\\\Documents\\\\F-VOICE\\\\wavs_text.txt',\n",
    "    'n_gpus':1,\n",
    "    'rank':0,\n",
    "    'group_name':None,\n",
    "    'text_cleaners': ['english_cleaners'],\n",
    "    ################################\n",
    "    # Audio Parameters             #\n",
    "    ################################\n",
    "    'max_wav_value': 32768.0,\n",
    "    'sampling_rate': 22050,\n",
    "    'load_mel_from_disk': False,\n",
    "    'filter_length': 1024,\n",
    "    'hop_length': 256,\n",
    "    'win_length': 1024,\n",
    "    'n_mel_channels': 80,\n",
    "    'mel_fmin': 0.0,\n",
    "    'mel_fmax': 8000.0,\n",
    "    'seed': 20,\n",
    "     ################################\n",
    "     # Optimization Hyperparameters #\n",
    "    ################################\n",
    "    'use_saved_learning_rate':False,\n",
    "    'learning_rate':1e-3,\n",
    "    'weight_decay':1e-6,\n",
    "    'grad_clip_thresh':1.0,\n",
    "    'batch_size':64,\n",
    "    'mask_padding':True,# set model's padded outputs to padded values\n",
    "    ################################\n",
    "    # Experiment Parameters        #\n",
    "    ################################\n",
    "    'epochs':500,\n",
    "    'iters_per_checkpoint':1000,\n",
    "    'dynamic_loss_scaling':True,\n",
    "    'fp16_run':False,\n",
    "    'distributed_run':False,\n",
    "    'dist_backend':\"gloo\",\n",
    "    'dist_url':\"tcp://localhost:54321\",\n",
    "    'cudnn_enabled':True,\n",
    "    'cudnn_benchmark':True,\n",
    "    'ignore_layers':['embedding.weight'],\n",
    "    'decay_start': 15000,\n",
    "    'min_learning_rate': 1e-5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 create Text from audio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtt.voice_to_text(hparams['in_audio_path'],hparams['out_audio_path'],language=\"english\") #Here creates the audio files to a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Audio To mels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.mel_creator(hparams['in_audio_path'],hparams['out_audio_mels_path']) #Here creates the audio files to Mel tensor's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = hparams['ouputh_checkpoint_path']+(r'/')+\"Model 0 F-VOICE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
